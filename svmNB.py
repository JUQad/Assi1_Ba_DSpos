# -*- coding: utf-8 -*-
"""Copy of svmNB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19a6AYsyoT6tU46Cs8Iwo5UPDO7ZqdZoG
"""

import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import model_selection, naive_bayes, svm
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

#import nltk
#nltk.download()
#from google.colab import drive
#drive.mount('/content/drive')


path         = "/Users/saharqaadan/Documents/bayer19/"
xlFile       = "sentences_with_sentiment.xlsx"
xl           = pd.ExcelFile (path + xlFile)
sheetName    = xl.sheet_names[0]
data1        = xl.parse(sheetName, header = 0)

data         = data1["Sentence"]
target       = data1["Class"] = data1["Positive"]*1 + data1["Negative"]*2 + data1["Neutral"]*3

df           = pd.DataFrame({'text':data, 'label':target})
print(type(df))
print(df.head(2))

# Step1: Data Pre-processing - This will help in getting better results through the classification algorithms
#Remove blank rows if any.
df['text'].dropna(inplace=True)

#Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently
df['text']  = [entry.lower() for entry in df['text']]

#import nltk
#nltk.download('punkt')
#Tokenization : In this each entry in the corpus will be broken into set of words
df['text']  = [word_tokenize(entry) for entry in df['text']]

#import nltk
#nltk.download('wordnet')
# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun
tag_map       = defaultdict(lambda : wn.NOUN)
tag_map['J']  = wn.ADJ
tag_map['V']  = wn.VERB
tag_map['R']  = wn.ADV

#import nltk
#nltk.download('averaged_perceptron_tagger')
#nltk.download('stopwords')

for index,entry in enumerate(df['text']):
    # Declaring Empty List to store the words that follow the rules for this step
    Final_words         = []
    word_Lemmatized     = WordNetLemmatizer()
    for word, tag in pos_tag(entry):
        if word not in stopwords.words('english') and word.isalpha():
            word_Final  = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
            Final_words.append(word_Final)
    # The final processed set of words for each iteration will be stored in 'text_final'
    df.loc[index,'textdf_final'] = str(Final_words)


# Step2: Split the model 
Traindf_X, Testdf_X, Traindf_Y, Testdf_Y = model_selection.train_test_split(df['textdf_final'],df['label'],test_size=0.3, random_state = 5)

print(Traindf_X.shape, Testdf_X.shape, Traindf_Y.shape, Testdf_Y.shape)



# Step3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values
Encoderdf       = LabelEncoder()
Traindf_Y       = Encoderdf.fit_transform(Traindf_Y)
Testdf_Y        = Encoderdf.fit_transform(Testdf_Y)

Tfidf_vectdf    = TfidfVectorizer(max_features=5000)
Tfidf_vectdf.fit(df['textdf_final'])
Train_X_Tfidfdf = Tfidf_vectdf.transform(Traindf_X)
Test_X_Tfidfdf  = Tfidf_vectdf.transform(Testdf_X)

print(Traindf_Y.shape, Testdf_Y.shape, Train_X_Tfidfdf.shape, Test_X_Tfidfdf.shape )

# Step4: Now we can run different algorithms to classify out data check for accuracy

# Classifier - Algorithm - Naive Bayes
from sklearn.model_selection import cross_val_score

Naivedf         = naive_bayes.MultinomialNB()
Naivedf.fit(Train_X_Tfidfdf,Traindf_Y)
scores          = cross_val_score(Naivedf, Train_X_Tfidfdf, Traindf_Y, cv=5)

print("********************************")
print("Accuracy of 5-CV in Naive-bayes : %0.2f (+/- %0.2f)" % (scores.mean()*100, scores.std() * 2))
predictionsdf_NB = Naivedf.predict(Test_X_Tfidfdf)
print('Naive Bayes df Accuracy Score ->'+str(accuracy_score(predictionsdf_NB, Testdf_Y)*100))
print('Naive Bayes --confusion_matrix ->' +str(confusion_matrix(predictionsdf_NB, Testdf_Y)))
print('F1-score: ' + str(f1_score(predictionsdf_NB, Testdf_Y, average='weighted'))  )

print("********************************")

# Classifier - Algorithm - SVM
# fit the training dataset on the classifier


SVMdf             = svm.SVC(C=5, kernel='linear', gamma='auto')
SVMdf.fit(Train_X_Tfidfdf,Traindf_Y)
scores            = cross_val_score(SVMdf, Train_X_Tfidfdf, Traindf_Y, cv=5)
print("Accuracy of 5-CV in SVM:: %0.2f (+/- %0.2f)" % (scores.mean()*100, scores.std() * 2))
predictionsdf_SVM = SVMdf.predict(Test_X_Tfidfdf)
print('SVM Accuracy Score -> '+ str(accuracy_score(predictionsdf_SVM, Testdf_Y)*100))
print('confusion_matrix ->' +str(confusion_matrix(predictionsdf_SVM, Testdf_Y)))
print('F1-score: ' + str(f1_score(predictionsdf_SVM, Testdf_Y, average='weighted')))

print("********************************")
